```python
%%capture
%pip install mne
%pip install pytorch-lightning
```


```python
import scipy.io
import torch.nn as nn
import torch
import numpy as np
```


```python
import numpy as np
import torch
import torch.nn as nn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report
from sklearn.model_selection import GroupKFold


food_path = r"C:\Users\34679\Desktop\TFM\EEG Prepro\S-1\Categorías Semánticas\tfr_food_data.npy"
kitchen_path = r"C:\Users\34679\Desktop\TFM\EEG Prepro\S-1\Categorías Semánticas\tfr_kitchen_data.npy"
animals_path = r"C:\Users\34679\Desktop\TFM\EEG Prepro\S-1\Categorías Semánticas\tfr_animals_data.npy"
instruments_path = r"C:\Users\34679\Desktop\TFM\EEG Prepro\S-1\Categorías Semánticas\tfr_music_data.npy"
body_parts_path = r"C:\Users\34679\Desktop\TFM\EEG Prepro\S-1\Categorías Semánticas\tfr_body_data.npy"
clothes_path = r"C:\Users\34679\Desktop\TFM\EEG Prepro\S-1\Categorías Semánticas\tfr_clothes_data.npy"


food_data = np.load(food_path)
kitchen_data = np.load(kitchen_path)
animals_data = np.load(animals_path)
instruments_data = np.load(instruments_path)
body_parts_data = np.load(body_parts_path)
clothes_data = np.load(clothes_path)

def convert_bool_to_int(data):
    if data.dtype == bool:
        data = data.astype(int)
    return data

def clean_data(data):
    data = np.nan_to_num(data, nan=0.0, posinf=0.0, neginf=0.0)
    return data


food_data = convert_bool_to_int(clean_data(food_data))
kitchen_data = convert_bool_to_int(clean_data(kitchen_data))
animals_data = convert_bool_to_int(clean_data(animals_data))
instruments_data = convert_bool_to_int(clean_data(instruments_data))
body_parts_data = convert_bool_to_int(clean_data(body_parts_data))
clothes_data = convert_bool_to_int(clean_data(clothes_data))


data_array = np.concatenate([food_data, kitchen_data, animals_data, instruments_data, body_parts_data, clothes_data], axis=0)


labels = np.array([0]*food_data.shape[0] + [1]*kitchen_data.shape[0] + [2]*animals_data.shape[0] +
                  [3]*instruments_data.shape[0] + [4]*body_parts_data.shape[0] + [5]*clothes_data.shape[0])



X_train, X_test, y_train, y_test = train_test_split(data_array, labels, test_size=0.2, random_state=42)


scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)
X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)

X_train = np.transpose(X_train, (0, 2, 1))
X_test = np.transpose(X_test, (0, 2, 1))

print("Dimensiones de X_train después de la transformación:", X_train.shape) 
print("Dimensiones de X_test después de la transformación:", X_test.shape) 


```

    Dimensiones de X_train después de la transformación: (297, 501, 10)
    Dimensiones de X_test después de la transformación: (75, 501, 10)
    


```python
# Definir la arquitectura del modelo ChronoNet
class Block(nn.Module):
    def __init__(self, inplace):
        super().__init__()
        self.conv1 = nn.Conv1d(in_channels=inplace, out_channels=32, kernel_size=2, stride=2, padding=0)
        self.conv2 = nn.Conv1d(in_channels=inplace, out_channels=32, kernel_size=4, stride=2, padding=1)
        self.conv3 = nn.Conv1d(in_channels=inplace, out_channels=32, kernel_size=8, stride=2, padding=3)
        self.relu = nn.ReLU()

    def forward(self, x):
        x1 = self.relu(self.conv1(x))
        x2 = self.relu(self.conv2(x))
        x3 = self.relu(self.conv3(x))
        x = torch.cat([x1, x2, x3], dim=1)
        return x

class ChronoNet(nn.Module):
    def __init__(self, channel):
        super().__init__()
        self.block1 = Block(channel)
        self.block2 = Block(96)
        self.block3 = Block(96)
        self.gru1 = nn.GRU(input_size=96, hidden_size=32, batch_first=True)
        self.gru2 = nn.GRU(input_size=32, hidden_size=32, batch_first=True)
        self.gru3 = nn.GRU(input_size=64, hidden_size=32, batch_first=True)
        self.gru4 = nn.GRU(input_size=64, hidden_size=32, batch_first=True)
        self.gru_linear = nn.Linear(96, 64) 
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(1984, 6) 
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.block1(x)
        x = self.block2(x)
        x = self.block3(x)
        x = x.permute(0, 2, 1) 
        gru_out1, _ = self.gru1(x)
        gru_out2, _ = self.gru2(gru_out1)
        gru_out = torch.cat([gru_out1, gru_out2], dim=2)  
        gru_out3, _ = self.gru3(gru_out)
        gru_out = torch.cat([gru_out1, gru_out2, gru_out3], dim=2)  
        linear_out = self.relu(self.gru_linear(gru_out))  
        gru_out4, _ = self.gru4(linear_out.permute(0, 1, 2))  
        x = self.flatten(gru_out4)
        x = self.fc1(x)
        return x

input = (10,501,62)
model = ChronoNet(10)
device = torch.device("cpu")
model.to(device)
```




    ChronoNet(
      (block1): Block(
        (conv1): Conv1d(10, 32, kernel_size=(2,), stride=(2,))
        (conv2): Conv1d(10, 32, kernel_size=(4,), stride=(2,), padding=(1,))
        (conv3): Conv1d(10, 32, kernel_size=(8,), stride=(2,), padding=(3,))
        (relu): ReLU()
      )
      (block2): Block(
        (conv1): Conv1d(96, 32, kernel_size=(2,), stride=(2,))
        (conv2): Conv1d(96, 32, kernel_size=(4,), stride=(2,), padding=(1,))
        (conv3): Conv1d(96, 32, kernel_size=(8,), stride=(2,), padding=(3,))
        (relu): ReLU()
      )
      (block3): Block(
        (conv1): Conv1d(96, 32, kernel_size=(2,), stride=(2,))
        (conv2): Conv1d(96, 32, kernel_size=(4,), stride=(2,), padding=(1,))
        (conv3): Conv1d(96, 32, kernel_size=(8,), stride=(2,), padding=(3,))
        (relu): ReLU()
      )
      (gru1): GRU(96, 32, batch_first=True)
      (gru2): GRU(32, 32, batch_first=True)
      (gru3): GRU(64, 32, batch_first=True)
      (gru4): GRU(64, 32, batch_first=True)
      (gru_linear): Linear(in_features=96, out_features=64, bias=True)
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (fc1): Linear(in_features=1984, out_features=6, bias=True)
      (relu): ReLU()
    )




```python
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

import seaborn as sns


sns.set(style="whitegrid")

train_losses = []
train_accuracies = []
# Ciclo de entrenamiento (ejemplo)
for epoch in range(50):
    model.train()
    optimizer.zero_grad()
    inputs = torch.tensor(X_train_scaled, dtype=torch.float32, device=device)
    targets = torch.tensor(y_train, dtype=torch.long, device=device)
    outputs = model(inputs)
    loss = criterion(outputs, targets)
    loss.backward()
    optimizer.step()
    print(f"Epoch [{epoch+1}/50], Loss: {loss.item():.4f}")

    # Guardar loss
    train_losses.append(loss.item())
    
    # Calcular accuracy en el conjunto de entrenamiento
    _, predicted_train = torch.max(outputs, 1)
    train_accuracy = (predicted_train == targets).sum().item() / len(targets)
    train_accuracies.append(train_accuracy)
    
    print(f"Epoch [{epoch+1}/50], Loss: {loss.item():.4f}, Accuracy: {train_accuracy * 100:.2f}%")



# Evaluación del modelo en el conjunto de prueba
model.eval()
inputs_test = torch.tensor(X_test_scaled, dtype=torch.float32, device=device)
targets_test = torch.tensor(y_test, dtype=torch.long, device=device)
with torch.no_grad():
    outputs_test = model(inputs_test)
    _, predicted_test = torch.max(outputs_test, 1)

# Métricas de evaluación
test_accuracy = (predicted_test == targets_test).sum().item() / len(targets_test)
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")

# Reporte de clasificación
print("\nClassification Report:")
print(classification_report(targets_test.cpu().numpy(), predicted_test.cpu().numpy()))
```

    Epoch [1/50], Loss: 1.7922
    Epoch [1/50], Loss: 1.7922, Accuracy: 15.15%
    Epoch [2/50], Loss: 1.8003
    Epoch [2/50], Loss: 1.8003, Accuracy: 17.85%
    Epoch [3/50], Loss: 1.7897
    Epoch [3/50], Loss: 1.7897, Accuracy: 18.18%
    Epoch [4/50], Loss: 1.7883
    Epoch [4/50], Loss: 1.7883, Accuracy: 18.86%
    Epoch [5/50], Loss: 1.7882
    Epoch [5/50], Loss: 1.7882, Accuracy: 17.85%
    Epoch [6/50], Loss: 1.7854
    Epoch [6/50], Loss: 1.7854, Accuracy: 17.85%
    Epoch [7/50], Loss: 1.7822
    Epoch [7/50], Loss: 1.7822, Accuracy: 18.52%
    Epoch [8/50], Loss: 1.7800
    Epoch [8/50], Loss: 1.7800, Accuracy: 18.52%
    Epoch [9/50], Loss: 1.7786
    Epoch [9/50], Loss: 1.7786, Accuracy: 18.52%
    Epoch [10/50], Loss: 1.7765
    Epoch [10/50], Loss: 1.7765, Accuracy: 18.52%
    Epoch [11/50], Loss: 1.7738
    Epoch [11/50], Loss: 1.7738, Accuracy: 18.86%
    Epoch [12/50], Loss: 1.7713
    Epoch [12/50], Loss: 1.7713, Accuracy: 18.86%
    Epoch [13/50], Loss: 1.7694
    Epoch [13/50], Loss: 1.7694, Accuracy: 19.53%
    Epoch [14/50], Loss: 1.7674
    Epoch [14/50], Loss: 1.7674, Accuracy: 19.53%
    Epoch [15/50], Loss: 1.7651
    Epoch [15/50], Loss: 1.7651, Accuracy: 19.53%
    Epoch [16/50], Loss: 1.7629
    Epoch [16/50], Loss: 1.7629, Accuracy: 19.53%
    Epoch [17/50], Loss: 1.7609
    Epoch [17/50], Loss: 1.7609, Accuracy: 19.53%
    Epoch [18/50], Loss: 1.7594
    Epoch [18/50], Loss: 1.7594, Accuracy: 19.53%
    Epoch [19/50], Loss: 1.7582
    Epoch [19/50], Loss: 1.7582, Accuracy: 19.53%
    Epoch [20/50], Loss: 1.7571
    Epoch [20/50], Loss: 1.7571, Accuracy: 19.53%
    Epoch [21/50], Loss: 1.7563
    Epoch [21/50], Loss: 1.7563, Accuracy: 19.53%
    Epoch [22/50], Loss: 1.7556
    Epoch [22/50], Loss: 1.7556, Accuracy: 19.53%
    Epoch [23/50], Loss: 1.7549
    Epoch [23/50], Loss: 1.7549, Accuracy: 19.53%
    Epoch [24/50], Loss: 1.7547
    Epoch [24/50], Loss: 1.7547, Accuracy: 19.53%
    Epoch [25/50], Loss: 1.7544
    Epoch [25/50], Loss: 1.7544, Accuracy: 19.53%
    Epoch [26/50], Loss: 1.7542
    Epoch [26/50], Loss: 1.7542, Accuracy: 19.87%
    Epoch [27/50], Loss: 1.7539
    Epoch [27/50], Loss: 1.7539, Accuracy: 19.53%
    Epoch [28/50], Loss: 1.7536
    Epoch [28/50], Loss: 1.7536, Accuracy: 19.53%
    Epoch [29/50], Loss: 1.7532
    Epoch [29/50], Loss: 1.7532, Accuracy: 20.54%
    Epoch [30/50], Loss: 1.7528
    Epoch [30/50], Loss: 1.7528, Accuracy: 19.87%
    Epoch [31/50], Loss: 1.7524
    Epoch [31/50], Loss: 1.7524, Accuracy: 20.54%
    Epoch [32/50], Loss: 1.7520
    Epoch [32/50], Loss: 1.7520, Accuracy: 19.87%
    Epoch [33/50], Loss: 1.7515
    Epoch [33/50], Loss: 1.7515, Accuracy: 20.54%
    Epoch [34/50], Loss: 1.7505
    Epoch [34/50], Loss: 1.7505, Accuracy: 25.59%
    Epoch [35/50], Loss: 1.7493
    Epoch [35/50], Loss: 1.7493, Accuracy: 21.55%
    Epoch [36/50], Loss: 1.7480
    Epoch [36/50], Loss: 1.7480, Accuracy: 25.25%
    Epoch [37/50], Loss: 1.7465
    Epoch [37/50], Loss: 1.7465, Accuracy: 26.94%
    Epoch [38/50], Loss: 1.7448
    Epoch [38/50], Loss: 1.7448, Accuracy: 25.93%
    Epoch [39/50], Loss: 1.7415
    Epoch [39/50], Loss: 1.7415, Accuracy: 26.94%
    Epoch [40/50], Loss: 1.7370
    Epoch [40/50], Loss: 1.7370, Accuracy: 26.94%
    Epoch [41/50], Loss: 1.7321
    Epoch [41/50], Loss: 1.7321, Accuracy: 27.27%
    Epoch [42/50], Loss: 1.7264
    Epoch [42/50], Loss: 1.7264, Accuracy: 28.62%
    Epoch [43/50], Loss: 1.7183
    Epoch [43/50], Loss: 1.7183, Accuracy: 28.96%
    Epoch [44/50], Loss: 1.7050
    Epoch [44/50], Loss: 1.7050, Accuracy: 33.33%
    Epoch [45/50], Loss: 1.6892
    Epoch [45/50], Loss: 1.6892, Accuracy: 34.01%
    Epoch [46/50], Loss: 1.6715
    Epoch [46/50], Loss: 1.6715, Accuracy: 33.00%
    Epoch [47/50], Loss: 1.6524
    Epoch [47/50], Loss: 1.6524, Accuracy: 38.05%
    Epoch [48/50], Loss: 1.6255
    Epoch [48/50], Loss: 1.6255, Accuracy: 34.68%
    Epoch [49/50], Loss: 1.5867
    Epoch [49/50], Loss: 1.5867, Accuracy: 40.40%
    Epoch [50/50], Loss: 1.5481
    Epoch [50/50], Loss: 1.5481, Accuracy: 41.08%
    Test Accuracy: 32.00%
    
    Classification Report:
                  precision    recall  f1-score   support
    
               0       0.46      0.38      0.41        16
               1       1.00      0.07      0.12        15
               2       0.11      0.20      0.14        10
               3       0.33      0.31      0.32        13
               4       0.33      0.56      0.42         9
               5       0.38      0.50      0.43        12
    
        accuracy                           0.32        75
       macro avg       0.44      0.33      0.31        75
    weighted avg       0.47      0.32      0.31        75
    
    


```python
from sklearn.model_selection import GroupKFold,LeaveOneGroupOut
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.utils import to_categorical
gkf=GroupKFold()
from sklearn.metrics import classification_report
```


```python
# If problems search for True print(np.isnan(X_train).any(), np.isnan(X_test).any())
# print(np.isinf(X_train).any(), np.isinf(X_test).any())
```


```python
import numpy as np
import torch
from sklearn.metrics import classification_report, roc_curve, auc, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from keras.utils import to_categorical  # Asumiendo que usas Keras para to_categorical

# Obtener predicciones del conjunto de prueba
model.eval()
with torch.no_grad():
    inputs_test = torch.tensor(X_test_scaled, dtype=torch.float32, device=device)
    test_predictions = model(inputs_test).cpu().numpy()  # Obtener las predicciones y pasarlas a CPU

test_pred_labels = np.argmax(test_predictions, axis=1)

# Mostrar el reporte de clasificación
print("\nClassification Report:")
print(classification_report(y_test, test_pred_labels))

# Accuracy por vocal
class_labels = ['Food', 'Kitchen', 'Animals', 'Instruments', 'Body Parts', 'Clothes']
for i, label in enumerate(class_labels):
    # Filtrar las predicciones y las etiquetas verdaderas para cada vocal
    true_labels = np.array(y_test) == i
    pred_labels = np.array(test_pred_labels) == i
    
    # Calcular el accuracy para cada vocal
    accuracy_per_class = np.sum(pred_labels & true_labels) / np.sum(true_labels)
    print(f"Accuracy para la vocal '{label}': {accuracy_per_class * 100:.2f}%")

# Curva ROC
num_classes = len(np.unique(y_test))
y_test_one_hot = to_categorical(y_test, num_classes=num_classes)

fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(num_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_one_hot[:, i], test_predictions[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

class_labels = ['Food', 'Kitchen', 'Animals', 'Instruments', 'Body Parts', 'Clothes']
plt.figure(figsize=(8, 6))
for i in range(num_classes):
    plt.plot(fpr[i], tpr[i], label=f'Vocal {class_labels[i]} (AUC = {roc_auc[i]:.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend()
plt.show()

# Matriz de confusión
conf_matrix = confusion_matrix(y_test, test_pred_labels)

plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel('Clase Predicha')
plt.ylabel('Clase Real')
plt.title('Matriz de Confusión')
plt.show()

```

    
    Classification Report:
                  precision    recall  f1-score   support
    
               0       0.46      0.38      0.41        16
               1       1.00      0.07      0.12        15
               2       0.11      0.20      0.14        10
               3       0.33      0.31      0.32        13
               4       0.33      0.56      0.42         9
               5       0.38      0.50      0.43        12
    
        accuracy                           0.32        75
       macro avg       0.44      0.33      0.31        75
    weighted avg       0.47      0.32      0.31        75
    
    Accuracy para la vocal 'Food': 37.50%
    Accuracy para la vocal 'Kitchen': 6.67%
    Accuracy para la vocal 'Animals': 20.00%
    Accuracy para la vocal 'Instruments': 30.77%
    Accuracy para la vocal 'Body Parts': 55.56%
    Accuracy para la vocal 'Clothes': 50.00%
    


    
![png](output_7_1.png)
    



    
![png](output_7_2.png)
    



```python
import matplotlib.pyplot as plt
import numpy as np
from scipy.ndimage import gaussian_filter1d
import seaborn as sns

# Configuración de estilo para Seaborn
sns.set(style="whitegrid")

# Asegúrate de que `train_losses` y `train_accuracies` contengan datos
if len(train_losses) == 0 or len(train_accuracies) == 0:
    raise ValueError("Las listas `train_losses` o `train_accuracies` están vacías.")

# Convertir listas a arrays de NumPy para suavizado y gráficos
train_losses = np.array(train_losses)
train_accuracies = np.array(train_accuracies)
epochs = np.arange(1, len(train_losses) + 1)

# Suavizado de datos para una apariencia similar a la imagen proporcionada
smoothed_losses = gaussian_filter1d(train_losses, sigma=2)
smoothed_accuracies = gaussian_filter1d(train_accuracies, sigma=2)

# Crear gráficos
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))

# Gráfico de precisión
ax1.plot(epochs, train_accuracies, 'o-', label='Training Accuracy', color='blue', markersize=4, alpha=0.6)
ax1.plot(epochs, smoothed_accuracies, label='Training Accuracy (smoothed)', color='blue')
ax1.set_ylabel('Accuracy (%)')
ax1.legend(loc='lower right')
ax1.grid(True)

# Gráfico de pérdida
ax2.plot(epochs, train_losses, 'o-', label='Training Loss', color='orange', markersize=4, alpha=0.6)
ax2.plot(epochs, smoothed_losses, label='Training Loss (smoothed)', color='orange')
ax2.set_ylabel('Loss')
ax2.set_xlabel('Epochs')
ax2.legend(loc='upper right')
ax2.grid(True)

plt.tight_layout()
plt.show()

```


    
![png](output_8_0.png)
    



```python
%pip install umap
from umap import UMAP
import matplotlib.pyplot as plt

softmax_outputs = model(torch.tensor(X_test_scaled, dtype=torch.float32, device=device)).detach().cpu().numpy()

true_labels = y_test

#Dimensions
umap_model = UMAP(n_components=3)
X_umap = umap_model.fit_transform(softmax_outputs)

fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
sc = ax.scatter(X_umap[:, 0], X_umap[:, 1], X_umap[:, 2], c=true_labels, cmap='viridis')
plt.colorbar(sc, label='Labels')
ax.set_xlabel('UMAP Component 1')
ax.set_ylabel('UMAP Component 2')
ax.set_zlabel('UMAP Component 3')
plt.title('UMAP de las salidas de Softmax en 3D')
plt.show()

```

    Requirement already satisfied: umap in c:\users\34679\appdata\local\programs\python\python312\lib\site-packages (0.1.1)
    Note: you may need to restart the kernel to use updated packages.
    


    
![png](output_9_1.png)
    



```python
import nbformat
from nbconvert import MarkdownExporter
import datetime
import os

# Función para exportar el notebook a Markdown con un nombre único
def export_notebook_to_markdown():
    # Cargar el notebook actual
    notebook_path = r"C:\Users\34679\Desktop\TFM\ChronoNet[3,3].ipynb"  # Reemplaza con la ruta de tu notebook
    with open(notebook_path, 'r', encoding='utf-8') as f:
        notebook = nbformat.read(f, as_version=4)
    
    # Configurar el exportador de Markdown
    md_exporter = MarkdownExporter()
    
    # Exportar el notebook a Markdown
    (body, resources) = md_exporter.from_notebook_node(notebook)
    
    # Crear un nombre de archivo único basado en la fecha y hora actuales
    
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f'Parti1_AllBands_{timestamp}.md'
    
    # Guardar el contenido Markdown en un archivo
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(body)
    
    print(f'Notebook exportado a {output_file}')

# Llamar a la función para exportar el notebook
export_notebook_to_markdown()

```

    Notebook exportado a Parti1_HighGamma_20240821_102454.md
    

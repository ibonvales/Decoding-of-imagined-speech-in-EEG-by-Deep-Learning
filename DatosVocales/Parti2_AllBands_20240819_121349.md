```python
%%capture
%pip install mne
%pip install pytorch-lightning
```

    Requirement already satisfied: pytorch-lightning in c:\users\34679\appdata\local\programs\python\python312\lib\site-packages (2.2.1)
    Requirement already satisfied: numpy>=1.17.2 in c:\users\34679\appdata\local\programs\python\python312\lib\site-packages (from pytorch-lightning) (1.26.4)
    Requirement already satisfied: torch>=1.13.0 in c:\users\34679\appdata\local\programs\python\python312\lib\site-packages (from pytorch-lightning) (2.2.2)
    Requirement already satisfied: tqdm>=4.57.0 in c:\users\34679\appdata\local\programs\python\python312\lib\site-packages (from pytorch-lightning) (4.66.2)
    Requirement already satisfied: PyYAML>=5.4 in c:\users\34679\appdata\local\programs\python\python312\lib\site-packages (from pytorch-lightning) (6.0.1)
    Requirement already satisfied: fsspec>=2022.5.0 in c:\users\34679\appdata\local\programs\python\python312\lib\site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.3.1)
    Requirement already satisfied: torchmetrics>=0.7.0 in c:\users\34679\appdata\local\programs\python\python312\lib\site-packages (from pytorch-lightning) (1.3.2)
    Requirement already satisfied: packaging>=20.0 in c:\users\34679\appdata\roaming\python\python312\site-packages (from pytorch-lightning) (23.2)
    Requirement already satisfied: typing-extensions>=4.4.0 in c:\users\34679\appdata\local\programs\python\python312\lib\site-packages (from pytorch-lightning) (4.10.0)
    Requirement already satisfied: lightning-utilities>=0.8.0 in c:\users\34679\appdata\local\programs\python\python312\lib\site-packages (from pytorch-lightning) (0.11.2)
    Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\users\34679\appdata\local\programs\python\python312\lib\site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.9.3)
    Requirement already satisfied: setuptools in c:\users\34679\appdata\local\programs\python\python312\lib\site-packages (from lightning-utilities>=0.8.0->pytorch-lightning) (69.1.1)
    Requirement already satisfied: filelock in c:\users\34679\appdata\local\programs\python\python312\lib\site-packages (from torch>=1.13.0->pytorch-lightning) (3.13.1)
    Requirement already satisfied: sympy in c:\users\34679\appdata\local\programs\python\python312\lib\site-packages (from torch>=1.13.0->pytorch-lightning) (1.12)
    Requirement already satisfied: networkx in c:\users\34679\appdata\local\programs\python\python312\lib\site-packages (from torch>=1.13.0->pytorch-lightning) (3.3)
    Requirement already satisfied: jinja2 in c:\users\34679\appdata\local\programs\python\python312\lib\site-packages (from torch>=1.13.0->pytorch-lightning) (3.1.3)
    Requirement already satisfied: colorama in c:\users\34679\appdata\roaming\python\python312\site-packages (from tqdm>=4.57.0->pytorch-lightning) (0.4.6)
    Requirement already satisfied: aiosignal>=1.1.2 in c:\users\34679\appdata\local\programs\python\python312\lib\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)
    Requirement already satisfied: attrs>=17.3.0 in c:\users\34679\appdata\local\programs\python\python312\lib\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (23.2.0)
    Requirement already satisfied: frozenlist>=1.1.1 in c:\users\34679\appdata\local\programs\python\python312\lib\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)
    Requirement already satisfied: multidict<7.0,>=4.5 in c:\users\34679\appdata\local\programs\python\python312\lib\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.5)
    Requirement already satisfied: yarl<2.0,>=1.0 in c:\users\34679\appdata\local\programs\python\python312\lib\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.4)
    Requirement already satisfied: MarkupSafe>=2.0 in c:\users\34679\appdata\local\programs\python\python312\lib\site-packages (from jinja2->torch>=1.13.0->pytorch-lightning) (2.1.5)
    Requirement already satisfied: mpmath>=0.19 in c:\users\34679\appdata\local\programs\python\python312\lib\site-packages (from sympy->torch>=1.13.0->pytorch-lightning) (1.3.0)
    Requirement already satisfied: idna>=2.0 in c:\users\34679\appdata\local\programs\python\python312\lib\site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.6)
    


```python
import scipy.io
import torch.nn as nn
import torch
import numpy as np
```


```python
import numpy as np
import torch
import torch.nn as nn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report
from sklearn.model_selection import GroupKFold

tfr_a_path = r"C:\Users\34679\Desktop\TFM\EEG Prepro\S-2\Morlet\tfr_a_data.npy"
tfr_e_path = r"C:\Users\34679\Desktop\TFM\EEG Prepro\S-2\Morlet\tfr_e_data.npy"
tfr_i_path = r"C:\Users\34679\Desktop\TFM\EEG Prepro\S-2\Morlet\tfr_i_data.npy"
tfr_o_path = r"C:\Users\34679\Desktop\TFM\EEG Prepro\S-2\Morlet\tfr_o_data.npy"
tfr_u_path = r"C:\Users\34679\Desktop\TFM\EEG Prepro\S-2\Morlet\tfr_u_data.npy"


tfr_a_data = np.load(tfr_a_path)
tfr_e_data = np.load(tfr_e_path)
tfr_i_data = np.load(tfr_i_path)
tfr_o_data = np.load(tfr_o_path)
tfr_u_data = np.load(tfr_u_path)

def convert_bool_to_int(data):
    if data.dtype == bool:
        data = data.astype(int)
    return data

def clean_data(data):
    data = np.nan_to_num(data, nan=0.0, posinf=0.0, neginf=0.0)
    return data


tfr_a_data = convert_bool_to_int(clean_data(tfr_a_data))
tfr_e_data = convert_bool_to_int(clean_data(tfr_e_data))
tfr_i_data = convert_bool_to_int(clean_data(tfr_i_data))
tfr_o_data = convert_bool_to_int(clean_data(tfr_o_data))
tfr_u_data = convert_bool_to_int(clean_data(tfr_u_data))


data_array = np.concatenate([tfr_a_data, tfr_e_data, tfr_i_data, tfr_o_data, tfr_u_data], axis=0)


labels = np.array([0]*tfr_a_data.shape[0] + [1]*tfr_e_data.shape[0] + [2]*tfr_i_data.shape[0] +
                  [3]*tfr_o_data.shape[0] + [4]*tfr_u_data.shape[0])


X_train, X_test, y_train, y_test = train_test_split(data_array, labels, test_size=0.2, random_state=42)


scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)
X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)

X_train = np.transpose(X_train, (0, 2, 1))
X_test = np.transpose(X_test, (0, 2, 1))

print("Dimensiones de X_train después de la transformación:", X_train.shape) 
print("Dimensiones de X_test después de la transformación:", X_test.shape) 


```

    Dimensiones de X_train después de la transformación: (232, 501, 10)
    Dimensiones de X_test después de la transformación: (58, 501, 10)
    


```python
# Definir la arquitectura del modelo ChronoNet
class Block(nn.Module):
    def __init__(self, inplace):
        super().__init__()
        self.conv1 = nn.Conv1d(in_channels=inplace, out_channels=32, kernel_size=2, stride=2, padding=0)
        self.conv2 = nn.Conv1d(in_channels=inplace, out_channels=32, kernel_size=4, stride=2, padding=1)
        self.conv3 = nn.Conv1d(in_channels=inplace, out_channels=32, kernel_size=8, stride=2, padding=3)
        self.relu = nn.ReLU()

    def forward(self, x):
        x1 = self.relu(self.conv1(x))
        x2 = self.relu(self.conv2(x))
        x3 = self.relu(self.conv3(x))
        x = torch.cat([x1, x2, x3], dim=1)
        return x

class ChronoNet(nn.Module):
    def __init__(self, channel):
        super().__init__()
        self.block1 = Block(channel)
        self.block2 = Block(96)
        self.block3 = Block(96)
        self.gru1 = nn.GRU(input_size=96, hidden_size=32, batch_first=True)
        self.gru2 = nn.GRU(input_size=32, hidden_size=32, batch_first=True)
        self.gru3 = nn.GRU(input_size=64, hidden_size=32, batch_first=True)
        self.gru4 = nn.GRU(input_size=64, hidden_size=32, batch_first=True)
        self.gru_linear = nn.Linear(96, 64) 
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(1984, 5) 
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.block1(x)
        x = self.block2(x)
        x = self.block3(x)
        x = x.permute(0, 2, 1) 
        gru_out1, _ = self.gru1(x)
        gru_out2, _ = self.gru2(gru_out1)
        gru_out = torch.cat([gru_out1, gru_out2], dim=2)  
        gru_out3, _ = self.gru3(gru_out)
        gru_out = torch.cat([gru_out1, gru_out2, gru_out3], dim=2)  
        linear_out = self.relu(self.gru_linear(gru_out))  
        gru_out4, _ = self.gru4(linear_out.permute(0, 1, 2))  
        x = self.flatten(gru_out4)
        x = self.fc1(x)
        return x

input = (10,501,62)
model = ChronoNet(10)
device = torch.device("cpu")
model.to(device)
```




    ChronoNet(
      (block1): Block(
        (conv1): Conv1d(10, 32, kernel_size=(2,), stride=(2,))
        (conv2): Conv1d(10, 32, kernel_size=(4,), stride=(2,), padding=(1,))
        (conv3): Conv1d(10, 32, kernel_size=(8,), stride=(2,), padding=(3,))
        (relu): ReLU()
      )
      (block2): Block(
        (conv1): Conv1d(96, 32, kernel_size=(2,), stride=(2,))
        (conv2): Conv1d(96, 32, kernel_size=(4,), stride=(2,), padding=(1,))
        (conv3): Conv1d(96, 32, kernel_size=(8,), stride=(2,), padding=(3,))
        (relu): ReLU()
      )
      (block3): Block(
        (conv1): Conv1d(96, 32, kernel_size=(2,), stride=(2,))
        (conv2): Conv1d(96, 32, kernel_size=(4,), stride=(2,), padding=(1,))
        (conv3): Conv1d(96, 32, kernel_size=(8,), stride=(2,), padding=(3,))
        (relu): ReLU()
      )
      (gru1): GRU(96, 32, batch_first=True)
      (gru2): GRU(32, 32, batch_first=True)
      (gru3): GRU(64, 32, batch_first=True)
      (gru4): GRU(64, 32, batch_first=True)
      (gru_linear): Linear(in_features=96, out_features=64, bias=True)
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (fc1): Linear(in_features=1984, out_features=5, bias=True)
      (relu): ReLU()
    )




```python
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

import seaborn as sns


sns.set(style="whitegrid")

train_losses = []
train_accuracies = []
# Ciclo de entrenamiento (ejemplo)
for epoch in range(50):
    model.train()
    optimizer.zero_grad()
    inputs = torch.tensor(X_train_scaled, dtype=torch.float32, device=device)
    targets = torch.tensor(y_train, dtype=torch.long, device=device)
    outputs = model(inputs)
    loss = criterion(outputs, targets)
    loss.backward()
    optimizer.step()
    print(f"Epoch [{epoch+1}/50], Loss: {loss.item():.4f}")

    # Guardar loss
    train_losses.append(loss.item())
    
    # Calcular accuracy en el conjunto de entrenamiento
    _, predicted_train = torch.max(outputs, 1)
    train_accuracy = (predicted_train == targets).sum().item() / len(targets)
    train_accuracies.append(train_accuracy)
    
    print(f"Epoch [{epoch+1}/30], Loss: {loss.item():.4f}, Accuracy: {train_accuracy * 100:.2f}%")



# Evaluación del modelo en el conjunto de prueba
model.eval()
inputs_test = torch.tensor(X_test_scaled, dtype=torch.float32, device=device)
targets_test = torch.tensor(y_test, dtype=torch.long, device=device)
with torch.no_grad():
    outputs_test = model(inputs_test)
    _, predicted_test = torch.max(outputs_test, 1)

# Métricas de evaluación
test_accuracy = (predicted_test == targets_test).sum().item() / len(targets_test)
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")

# Reporte de clasificación
print("\nClassification Report:")
print(classification_report(targets_test.cpu().numpy(), predicted_test.cpu().numpy()))
```

    Epoch [1/50], Loss: 1.5947
    Epoch [1/30], Loss: 1.5947, Accuracy: 20.26%
    Epoch [2/50], Loss: 1.5943
    Epoch [2/30], Loss: 1.5943, Accuracy: 27.16%
    Epoch [3/50], Loss: 1.5774
    Epoch [3/30], Loss: 1.5774, Accuracy: 23.28%
    Epoch [4/50], Loss: 1.5668
    Epoch [4/30], Loss: 1.5668, Accuracy: 26.29%
    Epoch [5/50], Loss: 1.5519
    Epoch [5/30], Loss: 1.5519, Accuracy: 25.86%
    Epoch [6/50], Loss: 1.5381
    Epoch [6/30], Loss: 1.5381, Accuracy: 29.31%
    Epoch [7/50], Loss: 1.5188
    Epoch [7/30], Loss: 1.5188, Accuracy: 33.19%
    Epoch [8/50], Loss: 1.4978
    Epoch [8/30], Loss: 1.4978, Accuracy: 33.62%
    Epoch [9/50], Loss: 1.4786
    Epoch [9/30], Loss: 1.4786, Accuracy: 34.91%
    Epoch [10/50], Loss: 1.4562
    Epoch [10/30], Loss: 1.4562, Accuracy: 34.48%
    Epoch [11/50], Loss: 1.4316
    Epoch [11/30], Loss: 1.4316, Accuracy: 37.50%
    Epoch [12/50], Loss: 1.4038
    Epoch [12/30], Loss: 1.4038, Accuracy: 38.79%
    Epoch [13/50], Loss: 1.3755
    Epoch [13/30], Loss: 1.3755, Accuracy: 39.22%
    Epoch [14/50], Loss: 1.3464
    Epoch [14/30], Loss: 1.3464, Accuracy: 40.52%
    Epoch [15/50], Loss: 1.3131
    Epoch [15/30], Loss: 1.3131, Accuracy: 40.95%
    Epoch [16/50], Loss: 1.2880
    Epoch [16/30], Loss: 1.2880, Accuracy: 42.24%
    Epoch [17/50], Loss: 1.2659
    Epoch [17/30], Loss: 1.2659, Accuracy: 44.40%
    Epoch [18/50], Loss: 1.2491
    Epoch [18/30], Loss: 1.2491, Accuracy: 50.86%
    Epoch [19/50], Loss: 1.2303
    Epoch [19/30], Loss: 1.2303, Accuracy: 55.17%
    Epoch [20/50], Loss: 1.2090
    Epoch [20/30], Loss: 1.2090, Accuracy: 52.16%
    Epoch [21/50], Loss: 1.1856
    Epoch [21/30], Loss: 1.1856, Accuracy: 53.02%
    Epoch [22/50], Loss: 1.1599
    Epoch [22/30], Loss: 1.1599, Accuracy: 62.93%
    Epoch [23/50], Loss: 1.1378
    Epoch [23/30], Loss: 1.1378, Accuracy: 58.19%
    Epoch [24/50], Loss: 1.1155
    Epoch [24/30], Loss: 1.1155, Accuracy: 56.47%
    Epoch [25/50], Loss: 1.0954
    Epoch [25/30], Loss: 1.0954, Accuracy: 62.50%
    Epoch [26/50], Loss: 1.0713
    Epoch [26/30], Loss: 1.0713, Accuracy: 62.50%
    Epoch [27/50], Loss: 1.0490
    Epoch [27/30], Loss: 1.0490, Accuracy: 61.64%
    Epoch [28/50], Loss: 1.0239
    Epoch [28/30], Loss: 1.0239, Accuracy: 61.21%
    Epoch [29/50], Loss: 1.0114
    Epoch [29/30], Loss: 1.0114, Accuracy: 62.93%
    Epoch [30/50], Loss: 1.0098
    Epoch [30/30], Loss: 1.0098, Accuracy: 60.78%
    Epoch [31/50], Loss: 0.9731
    Epoch [31/30], Loss: 0.9731, Accuracy: 63.79%
    Epoch [32/50], Loss: 0.9486
    Epoch [32/30], Loss: 0.9486, Accuracy: 63.79%
    Epoch [33/50], Loss: 0.9476
    Epoch [33/30], Loss: 0.9476, Accuracy: 63.79%
    Epoch [34/50], Loss: 0.9076
    Epoch [34/30], Loss: 0.9076, Accuracy: 69.83%
    Epoch [35/50], Loss: 0.9114
    Epoch [35/30], Loss: 0.9114, Accuracy: 65.95%
    Epoch [36/50], Loss: 0.8870
    Epoch [36/30], Loss: 0.8870, Accuracy: 66.81%
    Epoch [37/50], Loss: 0.8653
    Epoch [37/30], Loss: 0.8653, Accuracy: 68.97%
    Epoch [38/50], Loss: 0.8640
    Epoch [38/30], Loss: 0.8640, Accuracy: 66.38%
    Epoch [39/50], Loss: 0.8309
    Epoch [39/30], Loss: 0.8309, Accuracy: 71.55%
    Epoch [40/50], Loss: 0.8317
    Epoch [40/30], Loss: 0.8317, Accuracy: 68.97%
    Epoch [41/50], Loss: 0.8212
    Epoch [41/30], Loss: 0.8212, Accuracy: 71.12%
    Epoch [42/50], Loss: 0.7907
    Epoch [42/30], Loss: 0.7907, Accuracy: 74.14%
    Epoch [43/50], Loss: 0.7892
    Epoch [43/30], Loss: 0.7892, Accuracy: 69.83%
    Epoch [44/50], Loss: 0.7747
    Epoch [44/30], Loss: 0.7747, Accuracy: 74.14%
    Epoch [45/50], Loss: 0.7476
    Epoch [45/30], Loss: 0.7476, Accuracy: 74.14%
    Epoch [46/50], Loss: 0.7449
    Epoch [46/30], Loss: 0.7449, Accuracy: 71.98%
    Epoch [47/50], Loss: 0.7444
    Epoch [47/30], Loss: 0.7444, Accuracy: 71.98%
    Epoch [48/50], Loss: 0.7115
    Epoch [48/30], Loss: 0.7115, Accuracy: 74.14%
    Epoch [49/50], Loss: 0.6978
    Epoch [49/30], Loss: 0.6978, Accuracy: 75.86%
    Epoch [50/50], Loss: 0.6992
    Epoch [50/30], Loss: 0.6992, Accuracy: 76.29%
    Test Accuracy: 70.69%
    
    Classification Report:
                  precision    recall  f1-score   support
    
               0       0.60      0.55      0.57        11
               1       0.78      0.64      0.70        11
               2       0.53      0.83      0.65        12
               3       0.78      0.64      0.70        11
               4       1.00      0.85      0.92        13
    
        accuracy                           0.71        58
       macro avg       0.74      0.70      0.71        58
    weighted avg       0.74      0.71      0.71        58
    
    


```python
from sklearn.model_selection import GroupKFold,LeaveOneGroupOut
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.utils import to_categorical
gkf=GroupKFold()
from sklearn.metrics import classification_report
```


```python
# If problems search for True print(np.isnan(X_train).any(), np.isnan(X_test).any())
# print(np.isinf(X_train).any(), np.isinf(X_test).any())
```


```python
import numpy as np
import torch
from sklearn.metrics import classification_report, roc_curve, auc, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from keras.utils import to_categorical  # Asumiendo que usas Keras para to_categorical

# Obtener predicciones del conjunto de prueba
model.eval()
with torch.no_grad():
    inputs_test = torch.tensor(X_test_scaled, dtype=torch.float32, device=device)
    test_predictions = model(inputs_test).cpu().numpy()  # Obtener las predicciones y pasarlas a CPU

test_pred_labels = np.argmax(test_predictions, axis=1)

# Mostrar el reporte de clasificación
print("\nClassification Report:")
print(classification_report(y_test, test_pred_labels))

# Accuracy por vocal
class_labels = ['a', 'e', 'i', 'o', 'u']
for i, label in enumerate(class_labels):
    # Filtrar las predicciones y las etiquetas verdaderas para cada vocal
    true_labels = np.array(y_test) == i
    pred_labels = np.array(test_pred_labels) == i
    
    # Calcular el accuracy para cada vocal
    accuracy_per_class = np.sum(pred_labels & true_labels) / np.sum(true_labels)
    print(f"Accuracy para la vocal '{label}': {accuracy_per_class * 100:.2f}%")

# Curva ROC
num_classes = len(np.unique(y_test))
y_test_one_hot = to_categorical(y_test, num_classes=num_classes)

fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(num_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_one_hot[:, i], test_predictions[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

class_labels = ['a', 'e', 'i', 'o', 'u'] 
plt.figure(figsize=(8, 6))
for i in range(num_classes):
    plt.plot(fpr[i], tpr[i], label=f'Vocal {class_labels[i]} (AUC = {roc_auc[i]:.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend()
plt.show()

# Matriz de confusión
conf_matrix = confusion_matrix(y_test, test_pred_labels)

plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel('Clase Predicha')
plt.ylabel('Clase Real')
plt.title('Matriz de Confusión')
plt.show()

```

    
    Classification Report:
                  precision    recall  f1-score   support
    
               0       0.60      0.55      0.57        11
               1       0.78      0.64      0.70        11
               2       0.53      0.83      0.65        12
               3       0.78      0.64      0.70        11
               4       1.00      0.85      0.92        13
    
        accuracy                           0.71        58
       macro avg       0.74      0.70      0.71        58
    weighted avg       0.74      0.71      0.71        58
    
    Accuracy para la vocal 'a': 54.55%
    Accuracy para la vocal 'e': 63.64%
    Accuracy para la vocal 'i': 83.33%
    Accuracy para la vocal 'o': 63.64%
    Accuracy para la vocal 'u': 84.62%
    


    
![png](output_7_1.png)
    



    
![png](output_7_2.png)
    



```python
import matplotlib.pyplot as plt
import numpy as np
from scipy.ndimage import gaussian_filter1d
import seaborn as sns

# Configuración de estilo para Seaborn
sns.set(style="whitegrid")

# Asegúrate de que `train_losses` y `train_accuracies` contengan datos
if len(train_losses) == 0 or len(train_accuracies) == 0:
    raise ValueError("Las listas `train_losses` o `train_accuracies` están vacías.")

# Convertir listas a arrays de NumPy para suavizado y gráficos
train_losses = np.array(train_losses)
train_accuracies = np.array(train_accuracies)
epochs = np.arange(1, len(train_losses) + 1)

# Suavizado de datos para una apariencia similar a la imagen proporcionada
smoothed_losses = gaussian_filter1d(train_losses, sigma=2)
smoothed_accuracies = gaussian_filter1d(train_accuracies, sigma=2)

# Crear gráficos
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))

# Gráfico de precisión
ax1.plot(epochs, train_accuracies, 'o-', label='Training Accuracy', color='blue', markersize=4, alpha=0.6)
ax1.plot(epochs, smoothed_accuracies, label='Training Accuracy (smoothed)', color='blue')
ax1.set_ylabel('Accuracy (%)')
ax1.legend(loc='lower right')
ax1.grid(True)

# Gráfico de pérdida
ax2.plot(epochs, train_losses, 'o-', label='Training Loss', color='orange', markersize=4, alpha=0.6)
ax2.plot(epochs, smoothed_losses, label='Training Loss (smoothed)', color='orange')
ax2.set_ylabel('Loss')
ax2.set_xlabel('Epochs')
ax2.legend(loc='upper right')
ax2.grid(True)

plt.tight_layout()
plt.show()

```


    
![png](output_8_0.png)
    



```python
%pip install umap
from umap import UMAP
import matplotlib.pyplot as plt

softmax_outputs = model(torch.tensor(X_test_scaled, dtype=torch.float32, device=device)).detach().cpu().numpy()

true_labels = y_test

#Dimensions
umap_model = UMAP(n_components=3)
X_umap = umap_model.fit_transform(softmax_outputs)

fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
sc = ax.scatter(X_umap[:, 0], X_umap[:, 1], X_umap[:, 2], c=true_labels, cmap='viridis')
plt.colorbar(sc, label='Labels')
ax.set_xlabel('UMAP Component 1')
ax.set_ylabel('UMAP Component 2')
ax.set_zlabel('UMAP Component 3')
plt.title('UMAP de las salidas de Softmax en 3D')
plt.show()

```

    Requirement already satisfied: umap in c:\users\34679\appdata\local\programs\python\python312\lib\site-packages (0.1.1)
    Note: you may need to restart the kernel to use updated packages.
    


    
![png](output_9_1.png)
    



```python
import nbformat
from nbconvert import MarkdownExporter
import datetime
import os

# Función para exportar el notebook a Markdown con un nombre único
def export_notebook_to_markdown():
    # Cargar el notebook actual
    notebook_path = r"C:\Users\34679\Desktop\TFM\ChronoNet[3,3].ipynb"  # Reemplaza con la ruta de tu notebook
    with open(notebook_path, 'r', encoding='utf-8') as f:
        notebook = nbformat.read(f, as_version=4)
    
    # Configurar el exportador de Markdown
    md_exporter = MarkdownExporter()
    
    # Exportar el notebook a Markdown
    (body, resources) = md_exporter.from_notebook_node(notebook)
    
    # Crear un nombre de archivo único basado en la fecha y hora actuales
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f'Parti2_AllBands_{timestamp}.md'
    
    # Guardar el contenido Markdown en un archivo
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(body)
    
    print(f'Notebook exportado a {output_file}')

# Llamar a la función para exportar el notebook
export_notebook_to_markdown()

```

    Notebook exportado a Parti2_HighGamma_20240819_121118.md
    

```python
%%capture
%pip install mne
%pip install pytorch-lightning
```


```python
import scipy.io
import torch.nn as nn
import torch
import numpy as np
```


```python
import numpy as np
import torch
import torch.nn as nn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report
from sklearn.model_selection import GroupKFold

tfr_a_path = r"C:\Users\34679\Desktop\TFM\EEG Prepro\S-11\Morlet\tfr_a_data.npy"
tfr_e_path = r"C:\Users\34679\Desktop\TFM\EEG Prepro\S-11\Morlet\tfr_e_data.npy"
tfr_i_path = r"C:\Users\34679\Desktop\TFM\EEG Prepro\S-11\Morlet\tfr_i_data.npy"
tfr_o_path = r"C:\Users\34679\Desktop\TFM\EEG Prepro\S-11\Morlet\tfr_o_data.npy"
tfr_u_path = r"C:\Users\34679\Desktop\TFM\EEG Prepro\S-11\Morlet\tfr_u_data.npy"


tfr_a_data = np.load(tfr_a_path)
tfr_e_data = np.load(tfr_e_path)
tfr_i_data = np.load(tfr_i_path)
tfr_o_data = np.load(tfr_o_path)
tfr_u_data = np.load(tfr_u_path)

def convert_bool_to_int(data):
    if data.dtype == bool:
        data = data.astype(int)
    return data

def clean_data(data):
    data = np.nan_to_num(data, nan=0.0, posinf=0.0, neginf=0.0)
    return data


tfr_a_data = convert_bool_to_int(clean_data(tfr_a_data))
tfr_e_data = convert_bool_to_int(clean_data(tfr_e_data))
tfr_i_data = convert_bool_to_int(clean_data(tfr_i_data))
tfr_o_data = convert_bool_to_int(clean_data(tfr_o_data))
tfr_u_data = convert_bool_to_int(clean_data(tfr_u_data))


data_array = np.concatenate([tfr_a_data, tfr_e_data, tfr_i_data, tfr_o_data, tfr_u_data], axis=0)


labels = np.array([0]*tfr_a_data.shape[0] + [1]*tfr_e_data.shape[0] + [2]*tfr_i_data.shape[0] +
                  [3]*tfr_o_data.shape[0] + [4]*tfr_u_data.shape[0])


X_train, X_test, y_train, y_test = train_test_split(data_array, labels, test_size=0.2, random_state=42)


scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)
X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)

X_train = np.transpose(X_train, (0, 2, 1))
X_test = np.transpose(X_test, (0, 2, 1))

print("Dimensiones de X_train después de la transformación:", X_train.shape) 
print("Dimensiones de X_test después de la transformación:", X_test.shape) 


```

    Dimensiones de X_train después de la transformación: (248, 501, 10)
    Dimensiones de X_test después de la transformación: (62, 501, 10)
    


```python
# Definir la arquitectura del modelo ChronoNet
class Block(nn.Module):
    def __init__(self, inplace):
        super().__init__()
        self.conv1 = nn.Conv1d(in_channels=inplace, out_channels=32, kernel_size=2, stride=2, padding=0)
        self.conv2 = nn.Conv1d(in_channels=inplace, out_channels=32, kernel_size=4, stride=2, padding=1)
        self.conv3 = nn.Conv1d(in_channels=inplace, out_channels=32, kernel_size=8, stride=2, padding=3)
        self.relu = nn.ReLU()

    def forward(self, x):
        x1 = self.relu(self.conv1(x))
        x2 = self.relu(self.conv2(x))
        x3 = self.relu(self.conv3(x))
        x = torch.cat([x1, x2, x3], dim=1)
        return x

class ChronoNet(nn.Module):
    def __init__(self, channel):
        super().__init__()
        self.block1 = Block(channel)
        self.block2 = Block(96)
        self.block3 = Block(96)
        self.gru1 = nn.GRU(input_size=96, hidden_size=32, batch_first=True)
        self.gru2 = nn.GRU(input_size=32, hidden_size=32, batch_first=True)
        self.gru3 = nn.GRU(input_size=64, hidden_size=32, batch_first=True)
        self.gru4 = nn.GRU(input_size=64, hidden_size=32, batch_first=True)
        self.gru_linear = nn.Linear(96, 64) 
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(1984, 5) 
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.block1(x)
        x = self.block2(x)
        x = self.block3(x)
        x = x.permute(0, 2, 1) 
        gru_out1, _ = self.gru1(x)
        gru_out2, _ = self.gru2(gru_out1)
        gru_out = torch.cat([gru_out1, gru_out2], dim=2)  
        gru_out3, _ = self.gru3(gru_out)
        gru_out = torch.cat([gru_out1, gru_out2, gru_out3], dim=2)  
        linear_out = self.relu(self.gru_linear(gru_out))  
        gru_out4, _ = self.gru4(linear_out.permute(0, 1, 2))  
        x = self.flatten(gru_out4)
        x = self.fc1(x)
        return x

input = (10,501,62)
model = ChronoNet(10)
device = torch.device("cpu")
model.to(device)
```




    ChronoNet(
      (block1): Block(
        (conv1): Conv1d(10, 32, kernel_size=(2,), stride=(2,))
        (conv2): Conv1d(10, 32, kernel_size=(4,), stride=(2,), padding=(1,))
        (conv3): Conv1d(10, 32, kernel_size=(8,), stride=(2,), padding=(3,))
        (relu): ReLU()
      )
      (block2): Block(
        (conv1): Conv1d(96, 32, kernel_size=(2,), stride=(2,))
        (conv2): Conv1d(96, 32, kernel_size=(4,), stride=(2,), padding=(1,))
        (conv3): Conv1d(96, 32, kernel_size=(8,), stride=(2,), padding=(3,))
        (relu): ReLU()
      )
      (block3): Block(
        (conv1): Conv1d(96, 32, kernel_size=(2,), stride=(2,))
        (conv2): Conv1d(96, 32, kernel_size=(4,), stride=(2,), padding=(1,))
        (conv3): Conv1d(96, 32, kernel_size=(8,), stride=(2,), padding=(3,))
        (relu): ReLU()
      )
      (gru1): GRU(96, 32, batch_first=True)
      (gru2): GRU(32, 32, batch_first=True)
      (gru3): GRU(64, 32, batch_first=True)
      (gru4): GRU(64, 32, batch_first=True)
      (gru_linear): Linear(in_features=96, out_features=64, bias=True)
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (fc1): Linear(in_features=1984, out_features=5, bias=True)
      (relu): ReLU()
    )




```python
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

import seaborn as sns


sns.set(style="whitegrid")

train_losses = []
train_accuracies = []
# Ciclo de entrenamiento (ejemplo)
for epoch in range(50):
    model.train()
    optimizer.zero_grad()
    inputs = torch.tensor(X_train_scaled, dtype=torch.float32, device=device)
    targets = torch.tensor(y_train, dtype=torch.long, device=device)
    outputs = model(inputs)
    loss = criterion(outputs, targets)
    loss.backward()
    optimizer.step()
    print(f"Epoch [{epoch+1}/50], Loss: {loss.item():.4f}")

    # Guardar loss
    train_losses.append(loss.item())
    
    # Calcular accuracy en el conjunto de entrenamiento
    _, predicted_train = torch.max(outputs, 1)
    train_accuracy = (predicted_train == targets).sum().item() / len(targets)
    train_accuracies.append(train_accuracy)
    
    print(f"Epoch [{epoch+1}/50], Loss: {loss.item():.4f}, Accuracy: {train_accuracy * 100:.2f}%")



# Evaluación del modelo en el conjunto de prueba
model.eval()
inputs_test = torch.tensor(X_test_scaled, dtype=torch.float32, device=device)
targets_test = torch.tensor(y_test, dtype=torch.long, device=device)
with torch.no_grad():
    outputs_test = model(inputs_test)
    _, predicted_test = torch.max(outputs_test, 1)

# Métricas de evaluación
test_accuracy = (predicted_test == targets_test).sum().item() / len(targets_test)
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")

# Reporte de clasificación
print("\nClassification Report:")
print(classification_report(targets_test.cpu().numpy(), predicted_test.cpu().numpy()))
```

    Epoch [1/50], Loss: 1.6094
    Epoch [1/50], Loss: 1.6094, Accuracy: 20.16%
    Epoch [2/50], Loss: 1.6159
    Epoch [2/50], Loss: 1.6159, Accuracy: 26.21%
    Epoch [3/50], Loss: 1.5996
    Epoch [3/50], Loss: 1.5996, Accuracy: 32.66%
    Epoch [4/50], Loss: 1.5896
    Epoch [4/50], Loss: 1.5896, Accuracy: 23.39%
    Epoch [5/50], Loss: 1.5737
    Epoch [5/50], Loss: 1.5737, Accuracy: 27.42%
    Epoch [6/50], Loss: 1.5470
    Epoch [6/50], Loss: 1.5470, Accuracy: 35.08%
    Epoch [7/50], Loss: 1.5155
    Epoch [7/50], Loss: 1.5155, Accuracy: 39.52%
    Epoch [8/50], Loss: 1.4819
    Epoch [8/50], Loss: 1.4819, Accuracy: 40.73%
    Epoch [9/50], Loss: 1.4412
    Epoch [9/50], Loss: 1.4412, Accuracy: 44.35%
    Epoch [10/50], Loss: 1.3952
    Epoch [10/50], Loss: 1.3952, Accuracy: 48.79%
    Epoch [11/50], Loss: 1.3481
    Epoch [11/50], Loss: 1.3481, Accuracy: 52.02%
    Epoch [12/50], Loss: 1.2990
    Epoch [12/50], Loss: 1.2990, Accuracy: 53.63%
    Epoch [13/50], Loss: 1.2505
    Epoch [13/50], Loss: 1.2505, Accuracy: 54.03%
    Epoch [14/50], Loss: 1.1992
    Epoch [14/50], Loss: 1.1992, Accuracy: 63.71%
    Epoch [15/50], Loss: 1.1420
    Epoch [15/50], Loss: 1.1420, Accuracy: 59.68%
    Epoch [16/50], Loss: 1.0731
    Epoch [16/50], Loss: 1.0731, Accuracy: 60.48%
    Epoch [17/50], Loss: 0.9994
    Epoch [17/50], Loss: 0.9994, Accuracy: 65.73%
    Epoch [18/50], Loss: 0.9239
    Epoch [18/50], Loss: 0.9239, Accuracy: 70.97%
    Epoch [19/50], Loss: 0.8633
    Epoch [19/50], Loss: 0.8633, Accuracy: 73.79%
    Epoch [20/50], Loss: 0.8006
    Epoch [20/50], Loss: 0.8006, Accuracy: 76.61%
    Epoch [21/50], Loss: 0.7361
    Epoch [21/50], Loss: 0.7361, Accuracy: 79.03%
    Epoch [22/50], Loss: 0.6687
    Epoch [22/50], Loss: 0.6687, Accuracy: 81.05%
    Epoch [23/50], Loss: 0.6004
    Epoch [23/50], Loss: 0.6004, Accuracy: 81.85%
    Epoch [24/50], Loss: 0.5265
    Epoch [24/50], Loss: 0.5265, Accuracy: 86.29%
    Epoch [25/50], Loss: 0.4605
    Epoch [25/50], Loss: 0.4605, Accuracy: 87.90%
    Epoch [26/50], Loss: 0.4127
    Epoch [26/50], Loss: 0.4127, Accuracy: 88.71%
    Epoch [27/50], Loss: 0.3722
    Epoch [27/50], Loss: 0.3722, Accuracy: 90.32%
    Epoch [28/50], Loss: 0.3366
    Epoch [28/50], Loss: 0.3366, Accuracy: 88.71%
    Epoch [29/50], Loss: 0.3012
    Epoch [29/50], Loss: 0.3012, Accuracy: 90.32%
    Epoch [30/50], Loss: 0.2737
    Epoch [30/50], Loss: 0.2737, Accuracy: 92.34%
    Epoch [31/50], Loss: 0.2422
    Epoch [31/50], Loss: 0.2422, Accuracy: 93.55%
    Epoch [32/50], Loss: 0.2248
    Epoch [32/50], Loss: 0.2248, Accuracy: 93.55%
    Epoch [33/50], Loss: 0.2047
    Epoch [33/50], Loss: 0.2047, Accuracy: 94.35%
    Epoch [34/50], Loss: 0.1889
    Epoch [34/50], Loss: 0.1889, Accuracy: 94.35%
    Epoch [35/50], Loss: 0.1767
    Epoch [35/50], Loss: 0.1767, Accuracy: 95.16%
    Epoch [36/50], Loss: 0.1610
    Epoch [36/50], Loss: 0.1610, Accuracy: 93.95%
    Epoch [37/50], Loss: 0.1474
    Epoch [37/50], Loss: 0.1474, Accuracy: 95.56%
    Epoch [38/50], Loss: 0.1366
    Epoch [38/50], Loss: 0.1366, Accuracy: 95.97%
    Epoch [39/50], Loss: 0.1342
    Epoch [39/50], Loss: 0.1342, Accuracy: 95.56%
    Epoch [40/50], Loss: 0.1476
    Epoch [40/50], Loss: 0.1476, Accuracy: 95.16%
    Epoch [41/50], Loss: 0.1447
    Epoch [41/50], Loss: 0.1447, Accuracy: 94.76%
    Epoch [42/50], Loss: 0.1264
    Epoch [42/50], Loss: 0.1264, Accuracy: 96.37%
    Epoch [43/50], Loss: 0.1121
    Epoch [43/50], Loss: 0.1121, Accuracy: 95.97%
    Epoch [44/50], Loss: 0.1143
    Epoch [44/50], Loss: 0.1143, Accuracy: 96.37%
    Epoch [45/50], Loss: 0.1025
    Epoch [45/50], Loss: 0.1025, Accuracy: 95.97%
    Epoch [46/50], Loss: 0.1042
    Epoch [46/50], Loss: 0.1042, Accuracy: 95.97%
    Epoch [47/50], Loss: 0.0898
    Epoch [47/50], Loss: 0.0898, Accuracy: 96.37%
    Epoch [48/50], Loss: 0.0934
    Epoch [48/50], Loss: 0.0934, Accuracy: 96.77%
    Epoch [49/50], Loss: 0.0807
    Epoch [49/50], Loss: 0.0807, Accuracy: 96.37%
    Epoch [50/50], Loss: 0.0797
    Epoch [50/50], Loss: 0.0797, Accuracy: 96.77%
    Test Accuracy: 98.39%
    
    Classification Report:
                  precision    recall  f1-score   support
    
               0       1.00      1.00      1.00        13
               1       1.00      1.00      1.00        15
               2       0.92      1.00      0.96        11
               3       1.00      1.00      1.00        12
               4       1.00      0.91      0.95        11
    
        accuracy                           0.98        62
       macro avg       0.98      0.98      0.98        62
    weighted avg       0.99      0.98      0.98        62
    
    


```python
from sklearn.model_selection import GroupKFold,LeaveOneGroupOut
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.utils import to_categorical
gkf=GroupKFold()
from sklearn.metrics import classification_report
```


```python
# If problems search for True print(np.isnan(X_train).any(), np.isnan(X_test).any())
# print(np.isinf(X_train).any(), np.isinf(X_test).any())
```


```python
import numpy as np
import torch
from sklearn.metrics import classification_report, roc_curve, auc, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from keras.utils import to_categorical  # Asumiendo que usas Keras para to_categorical

# Obtener predicciones del conjunto de prueba
model.eval()
with torch.no_grad():
    inputs_test = torch.tensor(X_test_scaled, dtype=torch.float32, device=device)
    test_predictions = model(inputs_test).cpu().numpy()  # Obtener las predicciones y pasarlas a CPU

test_pred_labels = np.argmax(test_predictions, axis=1)

# Mostrar el reporte de clasificación
print("\nClassification Report:")
print(classification_report(y_test, test_pred_labels))

# Accuracy por vocal
class_labels = ['a', 'e', 'i', 'o', 'u']
for i, label in enumerate(class_labels):
    # Filtrar las predicciones y las etiquetas verdaderas para cada vocal
    true_labels = np.array(y_test) == i
    pred_labels = np.array(test_pred_labels) == i
    
    # Calcular el accuracy para cada vocal
    accuracy_per_class = np.sum(pred_labels & true_labels) / np.sum(true_labels)
    print(f"Accuracy para la vocal '{label}': {accuracy_per_class * 100:.2f}%")

# Curva ROC
num_classes = len(np.unique(y_test))
y_test_one_hot = to_categorical(y_test, num_classes=num_classes)

fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(num_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_one_hot[:, i], test_predictions[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

class_labels = ['a', 'e', 'i', 'o', 'u'] 
plt.figure(figsize=(8, 6))
for i in range(num_classes):
    plt.plot(fpr[i], tpr[i], label=f'Vocal {class_labels[i]} (AUC = {roc_auc[i]:.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend()
plt.show()

# Matriz de confusión
conf_matrix = confusion_matrix(y_test, test_pred_labels)

plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel('Clase Predicha')
plt.ylabel('Clase Real')
plt.title('Matriz de Confusión')
plt.show()

```

    
    Classification Report:
                  precision    recall  f1-score   support
    
               0       1.00      1.00      1.00        13
               1       1.00      1.00      1.00        15
               2       0.92      1.00      0.96        11
               3       1.00      1.00      1.00        12
               4       1.00      0.91      0.95        11
    
        accuracy                           0.98        62
       macro avg       0.98      0.98      0.98        62
    weighted avg       0.99      0.98      0.98        62
    
    Accuracy para la vocal 'a': 100.00%
    Accuracy para la vocal 'e': 100.00%
    Accuracy para la vocal 'i': 100.00%
    Accuracy para la vocal 'o': 100.00%
    Accuracy para la vocal 'u': 90.91%
    


    
![png](output_7_1.png)
    



    
![png](output_7_2.png)
    



```python
import matplotlib.pyplot as plt
import numpy as np
from scipy.ndimage import gaussian_filter1d
import seaborn as sns

# Configuración de estilo para Seaborn
sns.set(style="whitegrid")

# Asegúrate de que `train_losses` y `train_accuracies` contengan datos
if len(train_losses) == 0 or len(train_accuracies) == 0:
    raise ValueError("Las listas `train_losses` o `train_accuracies` están vacías.")

# Convertir listas a arrays de NumPy para suavizado y gráficos
train_losses = np.array(train_losses)
train_accuracies = np.array(train_accuracies)
epochs = np.arange(1, len(train_losses) + 1)

# Suavizado de datos para una apariencia similar a la imagen proporcionada
smoothed_losses = gaussian_filter1d(train_losses, sigma=2)
smoothed_accuracies = gaussian_filter1d(train_accuracies, sigma=2)

# Crear gráficos
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))

# Gráfico de precisión
ax1.plot(epochs, train_accuracies, 'o-', label='Training Accuracy', color='blue', markersize=4, alpha=0.6)
ax1.plot(epochs, smoothed_accuracies, label='Training Accuracy (smoothed)', color='blue')
ax1.set_ylabel('Accuracy (%)')
ax1.legend(loc='lower right')
ax1.grid(True)

# Gráfico de pérdida
ax2.plot(epochs, train_losses, 'o-', label='Training Loss', color='orange', markersize=4, alpha=0.6)
ax2.plot(epochs, smoothed_losses, label='Training Loss (smoothed)', color='orange')
ax2.set_ylabel('Loss')
ax2.set_xlabel('Epochs')
ax2.legend(loc='upper right')
ax2.grid(True)

plt.tight_layout()
plt.show()

```


    
![png](output_8_0.png)
    



```python
%pip install umap
from umap import UMAP
import matplotlib.pyplot as plt

softmax_outputs = model(torch.tensor(X_test_scaled, dtype=torch.float32, device=device)).detach().cpu().numpy()

true_labels = y_test

#Dimensions
umap_model = UMAP(n_components=3)
X_umap = umap_model.fit_transform(softmax_outputs)

fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
sc = ax.scatter(X_umap[:, 0], X_umap[:, 1], X_umap[:, 2], c=true_labels, cmap='viridis')
plt.colorbar(sc, label='Labels')
ax.set_xlabel('UMAP Component 1')
ax.set_ylabel('UMAP Component 2')
ax.set_zlabel('UMAP Component 3')
plt.title('UMAP de las salidas de Softmax en 3D')
plt.show()

```

    Requirement already satisfied: umap in c:\users\34679\appdata\local\programs\python\python312\lib\site-packages (0.1.1)Note: you may need to restart the kernel to use updated packages.
    
    


    
![png](output_9_1.png)
    



```python
import nbformat
from nbconvert import MarkdownExporter
import datetime
import os

# Función para exportar el notebook a Markdown con un nombre único
def export_notebook_to_markdown():
    # Cargar el notebook actual
    notebook_path = r"C:\Users\34679\Desktop\TFM\ChronoNet[3,3].ipynb"  # Reemplaza con la ruta de tu notebook
    with open(notebook_path, 'r', encoding='utf-8') as f:
        notebook = nbformat.read(f, as_version=4)
    
    # Configurar el exportador de Markdown
    md_exporter = MarkdownExporter()
    
    # Exportar el notebook a Markdown
    (body, resources) = md_exporter.from_notebook_node(notebook)
    
    # Crear un nombre de archivo único basado en la fecha y hora actuales
    
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f'Parti11_AllBands_{timestamp}.md'
    
    # Guardar el contenido Markdown en un archivo
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(body)
    
    print(f'Notebook exportado a {output_file}')

# Llamar a la función para exportar el notebook
export_notebook_to_markdown()

```

    Notebook exportado a Parti11_HighGamma_20240819_213603.md
    
